---
title: "LLMにおけるガードレールについて"
emoji: "🚧"
type: "tech" 
topics: ["llm"]
published: true
---

## 💬はじめに：
大規模言語モデル（LLM）がもたらす画期的な進歩は、知識の創造、コミュニケーションの向上、そして情報アクセスの革新に貢献しています。しかしながら、これらの技術は同時に、情報の誤伝播、プライバシー侵害、偏見の増幅といったリスクも引き起こしています。これらの問題を緩和し、LLMのポテンシャルを安全に活用するためには、強固な「ガードレール」の構築が求められます。

この記事では、LLMとAIの安全性を確保するためのガードレール、および関連するツール、フレームワーク、サービスの最新の取り組みを紹介します。
## 🚧ガードレールの種類
:::message
LLMにおけるガードレールは、大きく分けて以下の5つのカテゴリーに分類されます。
:::

### 1. 倫理的ガードレール
> これらは、差別的、偏見的、または有害と見なされる可能性のある出力を防ぐために設計されています。倫理的な基準を守ることで、技術が社会的に受け入れられる基盤を築きます。

### 2. コンプライアンスガードレール
> モデルの出力がデータ保護やユーザーのプライバシー保護など、法的要件に準拠していることを保証するために設計されています。

### 3. コンテキストガードレール
> 特定の文脈には適さないものの、必ずしも明確な有害性や違法性を持たないテキストを生成することを防ぐためのガードレールです。

### 4. セキュリティガードレール
> 内部および外部のセキュリティ脅威から保護し、機密情報の不正な開示や誤情報の拡散を防ぐことを目的としています。

### 5. 適応型ガードレール
> これらは、ガードレール自体がモデルの進化に合わせて更新され、倫理的および法的な基準に沿って運用され続けるように設計されています。

## 🔧ツール、フレームワーク、およびサービス
:::message
以下のツールやフレームワークは、LLMとAIセーフティのためのガードレールを提供し、実践する上で重要な役割を果たします。
:::

### **[NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)**
> - LLMベースの会話アプリケーションにプログラムによって制御可能なガードレールを簡単に追加できるオープンソースツールキット
> - 会話システムに特化しており、会話の流れを制御するためのガードレールを提供している
> - 話題、安全性、セキュリティの大きく３つのカテゴリー
> - ex) この様な質問が来た時には、こういう返答をする

### **[Guardrails AI](https://www.guardrailsai.com/)**
> - 特定のリスクを検出、定量化、緩和するための入出力ガードをアプリケーションに追加するためのpythonフレームワーク
> - 様々なリスクに対する事前構築済のバリデータ（検証ツール）のコレクションを提供（[guardrails hub](https://hub.guardrailsai.com/)）しており、これらのを組み合わせることでLLMの入出力を監視する。
    >   - バリデータは、LLMの入出力をチャックし特定の基準やルールに従っているかを検証するツールのこと
> - バリデータ例) **Competitor Check**
>   - このバリデータは、競合他社が名指しで言及されないようにするために使用するもの。

### **[Moderation API(OpenAI)](https://platform.openai.com/docs/guides/moderation)**
> - 自動的にテキストコンテンツを分析し、不適切または有害なテキストを識別し、フィルタリングするAPI
    >   - 主に出力に対して不適切な内容を識別し対処することを目的としている。
> - 特にOpenAIのモデルに最適化されており、リアルタイムのモデレーションが可能である点、またAPIとして提供されるため、開発者が容易に既存のシステムに統合できる。

### **[Perspective API](https://perspectiveapi.com/)**
> - オンラインのテキストコンテンツを分析し、有害性や不快感を引き起こす可能性がある言葉を識別するGoogleが提供するツール。
> - 会話やコメントがどの程度攻撃的であるか、または不快であるかをスコアリングし、有害なコンテンツを事前に識別できる。
> - 特定の言葉やフレーズ、または特定のコンテキストに対する感度を調整することで、APIの応用範囲を自在に変更可能。

### **[Azure Content Moderato](https://learn.microsoft.com/ja-jp/azure/ai-services/content-moderator/overview)**
> - コンテンツモデレーションサービスであり、テキスト、画像、およびビデオコンテンツの自動分析とフィルタリング可能
> - AIと機械学習を駆使して、不適切な内容、有害な画像、暴力的または成人向けのビジュアルコンテンツ、不快な言葉遣いなどを識別

### **[Llama Guard](https://huggingface.co/meta-llama/LlamaGuard-7b)**
> - 人間とAIの会話に特化した入出力セーフガードモデル。安全上のリスク分類を行いLLMプロンプトと応答の分類を可能にする。
> - ユーザーの質問とAIの応答の両方を分析し、有害またはポリシーに違反するコンテンツを識別する機能を持つ。
> - 既存のモデレーションツールと比較して[高いパフォーマンスを示す](https://ar5iv.labs.arxiv.org/html/2312.06674v1)。
    >   - OpenAIのModeration APIとGoogleのPerspective APIやMicrosoftのAzure Content Safety APIと比較して
### **[Lakera Guard](https://www.lakera.ai/)**
> - 即座にLLMアプリケーションを安全・セキュリティの脅威から保護するツール
> - プロンプトインジェクション、データ損失、有害コンテンツなどのリスクに対応している。
> - 1行のコードで簡単に導入できる、毎日10万件以上の新しい脅威情報をデータベースに追加しており、これによりAIのセキュリティが日々強化される。


## 🤗おわりにorまとめ
これらのガードレールとツールは、技術の責任ある利用を確保し、情報の誤伝播、プライバシー侵害、偏見の増幅などのリスクを最小限に抑えるために重要です。技術開発者、ポリシーメーカー、社会全体がこれらの安全策を適用し、更新することで、AI技術の持つ可能性を安全に享受できる社会を実現できることを目指しています。この記事が、LLMとAIセーフティに関心を持つすべての人々にとって、知識の向上と実践的なガイダンスの提供に役立つことを願っています。









